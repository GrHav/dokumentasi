# -*- coding: utf-8 -*-
"""Laporan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DZDCy1gsvidbrA_fbPJVzv_kKfXAeNZ1

# Laporan Proyek Machine Learning - Robert Varian

# Data Understanding
Dataset yang digunakan adalah Telco Customer Churn Dataset, tersedia di Kaggle:
ðŸ”— https://www.kaggle.com/datasets/blastchar/telco-customer-churn

Dataset ini berisi informasi pelanggan dari perusahaan telekomunikasi fiktif.

## Variabel Utama:
* gender, SeniorCitizen, Partner, Dependents
* tenure, PhoneService, MultipleLines, InternetService
* Contract, PaymentMethod, MonthlyCharges, TotalCharges
* Target: Churn (Yes/No)

# Data Preparation

## Data Loading
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!pip install kaggle

!kaggle datasets download -d blastchar/telco-customer-churn

import zipfile
with zipfile.ZipFile("telco-customer-churn.zip", "r") as zip_ref:
    zip_ref.extractall("telco_data")

df = pd.read_csv("telco_data/WA_Fn-UseC_-Telco-Customer-Churn.csv")
df.head()

"""Melihat apakah pengguna akan churn atau tidak berdasarkan distribusinya

## Exploratory Data Analysis
"""

columns_to_plot = df.columns.tolist()
columns_to_plot.remove('customerID')
columns_to_plot.remove('Churn')

for col in columns_to_plot:
    plt.figure(figsize=(8, 5))
    if df[col].dtype == 'object' or df[col].nunique() < 10: # Categorical or low cardinality columns
        sns.countplot(data=df, x=col, hue='Churn')
        plt.title(f'Distribution of {col} by Churn')
        plt.xticks(rotation=45, ha='right')
    else: # Numerical columns
        sns.histplot(data=df, x=col, hue='Churn', kde=True)
        plt.title(f'Distribution of {col} by Churn')
    plt.tight_layout()
    plt.show()

def detect_and_visualize_outliers_iqr(df, column):
    """
    Detects outliers in a numerical column using the IQR method and visualizes the distribution.

    Args:
        df (pd.DataFrame): The input DataFrame.
        column (str): The name of the numerical column to analyze.
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

    print(f"Outliers detected in column '{column}' using IQR:")
    print(outliers[[column]])

    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[column])
    plt.title(f'Boxplot of {column} with Outliers')
    plt.xlabel(column)
    plt.show()

detect_and_visualize_outliers_iqr(df.copy(), 'MonthlyCharges')

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.dropna(subset=['TotalCharges'], inplace=True)
detect_and_visualize_outliers_iqr(df.copy(), 'TotalCharges')

"""## Data Cleaning"""

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.dropna(inplace=True)
df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)
df.drop(['customerID'], axis=1, inplace=True)
df = pd.get_dummies(df, drop_first=True)

"""## Feature Scalling"""

X = df.drop('Churn', axis=1)
y = df['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Modeling

## Model 1: Logistic Regression
"""

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)
y_pred_lr = log_reg.predict(X_test_scaled)

"""## Model 2: Random Forest"""

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

"""## Model 3: XGBoost"""

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

"""# Evaluation

"""

def evaluate_model(name, y_true, y_pred):
    return {
        'Model': name,
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred),
        'Recall': recall_score(y_true, y_pred),
        'F1-Score': f1_score(y_true, y_pred)
    }

results = [
    evaluate_model("Logistic Regression", y_test, y_pred_lr),
    evaluate_model("Random Forest", y_test, y_pred_rf),
    evaluate_model("XGBoost", y_test, y_pred_xgb)
]

results_df = pd.DataFrame(results)
results_df.sort_values(by="F1-Score", ascending=False)

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
models = results_df['Model']

x = np.arange(len(models))  # the label locations
width = 0.15  # the width of the bars

fig, ax = plt.subplots(figsize=(12, 7))

for i, metric in enumerate(metrics):
    rects = ax.bar(x + i*width, results_df[metric], width, label=metric)
    ax.bar_label(rects, padding=3, fmt='%.3f')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Score')
ax.set_title('Model Performance Comparison')
ax.set_xticks(x + width * (len(metrics) - 1) / 2)
ax.set_xticklabels(models)
ax.legend()

fig.tight_layout()
plt.show()

"""Telihat pada gambar diatas bahwa
## Logistic Regression
- Accuracy : 0.787
- Precision : 0.621
- Recall : 0.516
- F1-Score : 0.564

## Random Forest
- Accuracy : 0.785
- Precision : 0.627
- Recall : 0.476
- F1-Score : 0.541

## XGBoost
- Accuracy : 0.763
- Precision : 0.566
- Recall : 0.471
- F1-Score : 0.514

Logistic regression lebih cocok untuk kasus ini. Karena F1-Score yang didapatkan lebih tinggi dibandingkan model Random Forest dan XGBoost.
"""