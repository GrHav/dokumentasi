# -*- coding: utf-8 -*-
"""Laporan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DZDCy1gsvidbrA_fbPJVzv_kKfXAeNZ1

# Laporan Proyek Machine Learning - Robert Varian

# Data Understanding
Dataset yang digunakan adalah Telco Customer Churn Dataset, tersedia di Kaggle:
ğŸ”— https://www.kaggle.com/datasets/blastchar/telco-customer-churn

Dataset ini berisi informasi pelanggan dari perusahaan telekomunikasi fiktif.

## Variabel Utama:
* gender, SeniorCitizen, Partner, Dependents
* tenure, PhoneService, MultipleLines, InternetService
* Contract, PaymentMethod, MonthlyCharges, TotalCharges
* Target: Churn (Yes/No)

# Data Preparation

Melakukan import library
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

"""## Data Loading

Memasukkan kaggle.json untuk menggunakan api json
"""

from google.colab import files
files.upload()

"""memasukkan kaggle.json agar bisa digunakan"""

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""mendownload library kaggle"""

!pip install kaggle

"""mendownload dataset telco-customer-churn langsung dari kaggle"""

!kaggle datasets download -d blastchar/telco-customer-churn

"""membuka file zip yang telah di downloadkan"""

import zipfile
with zipfile.ZipFile("telco-customer-churn.zip", "r") as zip_ref:
    zip_ref.extractall("telco_data")

"""membuka file csv yang telah di download"""

df = pd.read_csv("telco_data/WA_Fn-UseC_-Telco-Customer-Churn.csv")
df.head()

"""Melihat apakah pengguna akan churn atau tidak berdasarkan distribusinya

## Exploratory Data Analysis

Menampilkan distribusi fitur beserta churn
"""

columns_to_plot = df.columns.tolist()
columns_to_plot.remove('customerID')
columns_to_plot.remove('Churn')

for col in columns_to_plot:
    plt.figure(figsize=(8, 5))
    if df[col].dtype == 'object' or df[col].nunique() < 10: # Categorical or low cardinality columns
        sns.countplot(data=df, x=col, hue='Churn')
        plt.title(f'Distribution of {col} by Churn')
        plt.xticks(rotation=45, ha='right')
    else: # Numerical columns
        sns.histplot(data=df, x=col, hue='Churn', kde=True)
        plt.title(f'Distribution of {col} by Churn')
    plt.tight_layout()
    plt.show()

"""Berdasarkan plot yang didapatkan diatas didapatkan kesimpulan bahwa churn dapat terjadi kapan saja tidak tergantung dari `monthly charges` atau `total charges`

melakukan deteksi outlier dan menampilkannya pada kolom `total charges` dengan `monthly charges`
"""

def detect_and_visualize_outliers_iqr(df, column):
    """
    Detects outliers in a numerical column using the IQR method and visualizes the distribution.

    Args:
        df (pd.DataFrame): The input DataFrame.
        column (str): The name of the numerical column to analyze.
    """
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

    print(f"Outliers detected in column '{column}' using IQR:")
    print(outliers[[column]])

    plt.figure(figsize=(10, 6))
    sns.boxplot(x=df[column])
    plt.title(f'Boxplot of {column} with Outliers')
    plt.xlabel(column)
    plt.show()

detect_and_visualize_outliers_iqr(df.copy(), 'MonthlyCharges')

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.dropna(subset=['TotalCharges'], inplace=True)
detect_and_visualize_outliers_iqr(df.copy(), 'TotalCharges')

"""Tidak ditemukan outlier pada kedua fitur tersebut

## Data Cleaning

Membersihkan data `total charges`. membuang data kosong, melakukan encode pada `churn`, menghapus `customerid`
"""

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df.dropna(inplace=True)
df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)
df.drop(['customerID'], axis=1, inplace=True)
df = pd.get_dummies(df, drop_first=True)

"""## Feature Scalling

Melakukan scalling pada data dengan size test 20%
"""

X = df.drop('Churn', axis=1)
y = df['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Modeling

## Model 1: Logistic Regression

Logistic Regression digunakan untuk klasifikasi biner, dan menghitung probabilitas suatu data termasuk ke dalam kelas positif (misalnya 1).

Rumus matematis:

ğ‘ƒ
(
ğ‘¦
=
1
âˆ£
ğ‘‹
)
=
1
1
+
ğ‘’
âˆ’
(
ğ›½
0
+
ğ›½
1
ğ‘¥
1
+
ğ›½
2
ğ‘¥
2
+
â‹¯
+
ğ›½
ğ‘›
ğ‘¥
ğ‘›
)
P(y=1âˆ£X)=
1+e
âˆ’(Î²
0
â€‹
 +Î²
1
â€‹
 x
1
â€‹
 +Î²
2
â€‹
 x
2
â€‹
 +â‹¯+Î²
n
â€‹
 x
n
â€‹
 )

1
â€‹

ğ‘‹
=
(
ğ‘¥
1
,
ğ‘¥
2
,
.
.
.
,
ğ‘¥
ğ‘›
)
X=(x
1
â€‹
 ,x
2
â€‹
 ,...,x
n
â€‹
 ): fitur input

ğ›½
=
(
ğ›½
0
,
ğ›½
1
,
.
.
.
,
ğ›½
ğ‘›
)
Î²=(Î²
0
â€‹
 ,Î²
1
â€‹
 ,...,Î²
n
â€‹
 ): koefisien model

Fungsi aktivasi yang digunakan adalah sigmoid:

ğœ
(
ğ‘§
)
=
1
1
+
ğ‘’
âˆ’
ğ‘§
Ïƒ(z)=
1+e
âˆ’z

1
â€‹

Prediksi kelas:

ğ‘¦
^
=
{
1
jika
ğ‘ƒ
(
ğ‘¦
=
1
âˆ£
ğ‘‹
)
>
0.5
0
jika
ğ‘ƒ
(
ğ‘¦
=
1
âˆ£
ğ‘‹
)
â‰¤
0.5
y
^
â€‹
 ={
1
0
â€‹
  
jikaÂ P(y=1âˆ£X)>0.5
jikaÂ P(y=1âˆ£X)â‰¤0.5
â€‹
"""

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)
y_pred_lr = log_reg.predict(X_test_scaled)

"""## Model 2: Random Forest

Random Forest adalah ensemble dari banyak decision tree, dan hasil prediksi ditentukan melalui voting (klasifikasi) atau rata-rata (regresi).

Prediksi klasifikasi:

ğ‘¦
^
=
mode
(
ğ‘‡
1
(
ğ‘‹
)
,
ğ‘‡
2
(
ğ‘‹
)
,
.
.
.
,
ğ‘‡
ğ‘˜
(
ğ‘‹
)
)
y
^
â€‹
 =mode(T
1
â€‹
 (X),T
2
â€‹
 (X),...,T
k
â€‹
 (X))
ğ‘‡
ğ‘–
(
ğ‘‹
)
* T
i
â€‹
 (X): prediksi dari decision tree ke-
ğ‘–
i

* Hasil akhir adalah mayoritas voting dari semua pohon.

Karakteristik:

* Menggunakan bagging (bootstrap aggregation)

* Mengurangi overfitting dibanding single decision tree
"""

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

"""## Model 3: XGBoost

XGBoost membangun model secara bertahap menggunakan boosting, menambahkan pohon baru untuk memperbaiki kesalahan pohon sebelumnya.

Rumus umum (untuk regresi):

ğ‘¦
^
ğ‘–
(
ğ‘¡
)
=
ğ‘¦
^
ğ‘–
(
ğ‘¡
âˆ’
1
)
+
ğœ‚
â‹…
ğ‘“
ğ‘¡
(
ğ‘¥
ğ‘–
)
y
^
â€‹
  
i
(t)
â€‹
 =
y
^
â€‹
  
i
(tâˆ’1)
â€‹
 +Î·â‹…f
t
â€‹
 (x
i
â€‹
 )
ğ‘¦
^
ğ‘–
(
ğ‘¡
)
y
^
â€‹
  
i
(t)
â€‹
 : prediksi pada iterasi ke-
ğ‘¡
t

ğ‘“
ğ‘¡
f
t
â€‹
 : pohon ke-
ğ‘¡
t

ğœ‚
Î·: learning rate

Objektif:

Obj
=
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘™
(
ğ‘¦
ğ‘–
,
ğ‘¦
^
ğ‘–
)
+
âˆ‘
ğ‘¡
=
1
ğ‘‡
Î©
(
ğ‘“
ğ‘¡
)
Obj=
i=1
âˆ‘
n
â€‹
 l(y
i
â€‹
 ,
y
^
â€‹
  
i
â€‹
 )+
t=1
âˆ‘
T
â€‹
 Î©(f
t
â€‹
 )
ğ‘™
l: loss function (misalnya MSE, log loss)

Î©
(
ğ‘“
ğ‘¡
)
Î©(f
t
â€‹
 ): regularisasi untuk mengontrol kompleksitas pohon:

Î©
(
ğ‘“
)
=
ğ›¾
ğ‘‡
+
1
2
ğœ†
âˆ‘
ğ‘—
=
1
ğ‘‡
ğ‘¤
ğ‘—
2
Î©(f)=Î³T+
2
1
â€‹
 Î»
j=1
âˆ‘
T
â€‹
 w
j
2
â€‹
"""

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

"""# Evaluation

Melakukan evaluasi data dari hasil yang didapatkan
"""

def evaluate_model(name, y_true, y_pred):
    return {
        'Model': name,
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred),
        'Recall': recall_score(y_true, y_pred),
        'F1-Score': f1_score(y_true, y_pred)
    }

results = [
    evaluate_model("Logistic Regression", y_test, y_pred_lr),
    evaluate_model("Random Forest", y_test, y_pred_rf),
    evaluate_model("XGBoost", y_test, y_pred_xgb)
]

results_df = pd.DataFrame(results)
results_df.sort_values(by="F1-Score", ascending=False)

"""Berdasarkan evaluasi metrik, dibuatkan visualisasi sebagai perbandingan antar model"""

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
models = results_df['Model']

x = np.arange(len(models))  # the label locations
width = 0.15  # the width of the bars

fig, ax = plt.subplots(figsize=(12, 7))

for i, metric in enumerate(metrics):
    rects = ax.bar(x + i*width, results_df[metric], width, label=metric)
    ax.bar_label(rects, padding=3, fmt='%.3f')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Score')
ax.set_title('Model Performance Comparison')
ax.set_xticks(x + width * (len(metrics) - 1) / 2)
ax.set_xticklabels(models)
ax.legend()

fig.tight_layout()
plt.show()

"""Telihat pada gambar diatas bahwa
## Logistic Regression
- Accuracy : 0.787
- Precision : 0.621
- Recall : 0.516
- F1-Score : 0.564

## Random Forest
- Accuracy : 0.785
- Precision : 0.627
- Recall : 0.476
- F1-Score : 0.541

## XGBoost
- Accuracy : 0.763
- Precision : 0.566
- Recall : 0.471
- F1-Score : 0.514

Logistic regression lebih cocok untuk kasus ini. Karena F1-Score yang didapatkan lebih tinggi dibandingkan model Random Forest dan XGBoost.
"""